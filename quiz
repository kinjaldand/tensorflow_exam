Learnings:  
1. Why standarization?
Case 1: Diff variables have diff ranges:
        - Features with larger values will dominate loss
Case 2: Diff variables have same range
        - The difference is that loss is decreasing at a faster rate then the case where standarization is not done
        Eg: With Standarization
            Epoch 1/5
            1875/1875 [==============================] - 8s 4ms/step - loss: 0.2024
            Epoch 2/5
            1875/1875 [==============================] - 8s 4ms/step - loss: 0.0808
            Epoch 3/5
            1875/1875 [==============================] - 8s 4ms/step - loss: 0.0533
            Epoch 4/5
            1875/1875 [==============================] - 8s 4ms/step - loss: 0.0368
            Epoch 5/5
            1875/1875 [==============================] - 8s 4ms/step - loss: 0.0280
            313/313 [==============================] - 1s 2ms/step - loss: 0.0651
            [1.2036349e-08 1.2807031e-08 8.8406367e-08 9.3468443e-05 1.5356870e-11 8.5729539e-08 2.4395022e-11 9.9989855e-01 3.2190380e-07 7.3837341e-06]
            7
          Eg: Without Standarization
              Epoch 1/5
              1875/1875 [==============================] - 8s 4ms/step - loss: 2.4753
              Epoch 2/5
              1875/1875 [==============================] - 8s 4ms/step - loss: 0.3192
              Epoch 3/5
              1875/1875 [==============================] - 9s 5ms/step - loss: 0.2990
              Epoch 4/5
              1875/1875 [==============================] - 7s 4ms/step - loss: 0.2446
              Epoch 5/5
              1875/1875 [==============================] - 7s 4ms/step - loss: 0.2388
              313/313 [==============================] - 1s 2ms/step - loss: 0.3455
              [2.5019048e-32 4.9267075e-12 4.4290942e-12 7.5214140e-14 1.6416640e-19 2.9890408e-21 5.9304127e-31 1.0000000e+00 2.1490184e-22 8.5462005e-19]
              7

2. Detect Overfitting:
        1. Diff between training and val accuracy
        

3. Convolution Output:
        - Reduced by 2 pixels because convolution starts at pixel (2,2) to (n-1,n-1) in 1..n pixels
        - Channels last  => Filters last eg: input_shape: (None,28,28,1)  =>Con2d(64,(3,3))  => (None,26,26,64)
        
4. Optimizers:
        sgd : constant learning rate
        ada, rmsprop, adam : automatically adapt the learning rate during training
5. Model.fit with generators:
        #steps_per_epoch : batch_size in "train_generator" =128, total_image_len =1000, steps_per_epoch=math.ciel(1000/128) = 8
6. Dropout:
        The idea behind Dropouts is that they remove a random number of neurons in your neural network. 
        This works very well for two reasons: 
                The first is that neighboring neurons often end up with similar weights, which can lead to overfitting, so dropping some out at random can remove this. 
                The second is that often a neuron can over-weigh the input from a neuron in the previous layer, and can over specialize as a result. 
        Thus, dropping out can break the neural network out of this potential bad habit!
        
7. Transfer Learning:
        Take first n layers, freeze them, add Dense layers on top and train!

8. Text to numbers:
        Tokenizer: Words to numbers
        
        
        


######################### Strategies ##################
1. Standarize Data
2. Implement callbacks for early stopping specially val_accuracy
3. Regulaziers/ Dropout ; first verify overfitting from training curves
4. Augmentation
5. Plot Acc and loss for train and val set

################# Module 1 ##############
Quiz 1
Answers,rules, labelling data, dense layer - connected neurons,
loss- measures how good ur guess is, optimizer - generates a new and improved guess, Convergence - process of getting close to correct answer ,
model.fit - trains NN

Quiz 2
What method gets called when an epoch finishes?
- on_epoch_end

What parameter to you set in your fit function to tell it to use callbacks?
- callbacks=

Quiz 3
Convolution: isolate features, reduce size,isolate features, 26*26,13*13, slower

Quiz 4
directory, rescale,training_size, 3 bytes colour, overfitting training, features different part of frame, less information 

################# Module 1 ##############
Quiz 1
all, 148*148, 75*75, history var return of model.fit, model.layers, 2 epcoh overfit train data, 
better as indicator with new images, less likelihood of all features being encountered

Quiz 2







