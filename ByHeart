#Imports

import tensorflow as tf
import numpy as np
from tensorflow import keras

##---------------- Define Data #----------------
a = np.array([],dtype=float)

#Reshape Data
training_images=training_images.reshape(60000, 28, 28, 1)
# datasets
keras.datasets.mnist
mnist = tf.keras.datasets.fashion_mnist

##---------------- Data Generators #----------------
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# All images will be rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1/255)

# Flow training images in batches of 128 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
        '/tmp/horse-or-human/',  # This is the source directory for training images
        target_size=(300, 300),  # All images will be resized to 150x150
        batch_size=128,
        class_mode='binary')  # Since we use binary_crossentropy loss, we need binary labels


##---------------- Model Preparation #--------------------

model = keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])
# Sequential takes list of layers
# layers =[Dense, Flatten,Conv2D]
# Dense Layer :  units=int, input_shape=tuple, activation =[tf.nn.relu, tf.nn.softmax,tf.nn.sigmoid]
# Conv Layer  :  num_filters=int, kernel_size = tuple, activation =[tf.nn.relu, tf.nn.softmax] , input_shape=tuple
# MaxPooling2D Layer : pool_size = tuple ,padding = ["same","valid"]
        output_shape = (input_shape - pool_size + 1) / strides)
        padding: "valid" adds no zero padding. "same" adds padding such that if the stride is 1, the output shape is the same as input shape.

##---------------- Model Compilation #------------------------
from tensorflow.keras.optimizers import RMSprop

model.compile(optimizer='sgd',loss='mean_squared_error')
# optimizer: [sgd,adam,rmsprop,ada,RMSprop(lr=0.001)]
        sgd : constant learning rate
        ada, rmsprop, adam : automatically adapt the learning rate during training
# loss: [mean_squared_error,sparse_categorical_crossentropy,binary_crossentropy]
        [linear regression, multiclass classification,binary classification]
# metrics: [accuracy]

##---------------- Model Training #----------------

history = model.fit(x,y,epochs=500)
# epochs=int

print(history.epoch, history.history['acc'][-1])

# Callbacks
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('loss')<0.4):
      print("\nReached 60% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()
model.fit(x, training_ylabels, epochs=5, callbacks=[callbacks])
# epochs=int
# callbacks=list of callbacks

##--------- Model Training with Generators #------------------
history = model.fit(
      train_generator,
      steps_per_epoch=8,  
      epochs=15,
      verbose=1)
#steps_per_epoch : batch_size in "train_generator" =128, total_image_len =1000, steps_per_epoch=math.ciel(1000/128) = 8

history = model.fit(
      train_generator,
      steps_per_epoch=8,  
      epochs=15,
      verbose=1,
      validation_data = validation_generator,
      validation_steps=8)


##---------------- Model Predict #----------------

model.predict(xval)
# Output Shape same as neurons in last layer
# 1 for regression, no. of classes for classification

##---------------- Model Evaluate #----------------

model.evaluate(xval,yval)
# outputs loss and accuracy

##---------------- Model Visualization #----------------
import matplotlib.pyplot as plt
f, axarr = plt.subplots(3,4)
FIRST_IMAGE=0
SECOND_IMAGE=7
THIRD_IMAGE=26
CONVOLUTION_NUMBER = 10
from tensorflow.keras import models
layer_outputs = [layer.output for layer in model.layers]
for layer in layer_outputs:
  print(layer.shape)

 
activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)
for x in range(0,4):
  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]
  print(f1.shape)
  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[0,x].grid(False)
  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[1,x].grid(False)
  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[2,x].grid(False)


##---------------- Plot Accuracy and Loss #----------------

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
acc      = history.history[     'accuracy' ]
val_acc  = history.history[ 'val_accuracy' ]
loss     = history.history[    'loss' ]
val_loss = history.history['val_loss' ]

epochs   = range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot  ( epochs,     acc )
plt.plot  ( epochs, val_acc )
plt.title ('Training and validation accuracy')
plt.figure()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot  ( epochs,     loss )
plt.plot  ( epochs, val_loss )
plt.title ('Training and validation loss'   )

##---------------- Misc
np.set_printoptions(linewidth=200)
import matplotlib.pyplot as plt
plt.imshow(training_images[0])


